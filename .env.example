# LLM Provider API Keys
OPENAI_API_KEY=sk-...
DEEPSEEK_API_KEY=sk-...
GROQ_API_KEY=gsk_...
HF_API_KEY=your_huggingface_api_key_here

# LLM Provider Base URLs
OPENAI_BASE_URL=https://api.openai.com/v1
DEEPSEEK_BASE_URL=https://api.deepseek.com/v1
GROQ_BASE_URL=https://api.groq.com/openai/v1
HF_BASE_URL=https://api-inference.huggingface.co/models

# Default Configuration
DEFAULT_MODEL=gpt-4o  # Will be overridden by provider-specific model
BATCH_SIZE=25
MAX_RETRIES=3
TEMPERATURE=0.7

# Available Models by Provider (Latest as of March 2024):

# OpenAI Latest Models:
# - gpt-4o (2024-11-20) [Latest large GA model, 128k context]
# - gpt-4o-mini (2024-07-18) [Latest small GA model, 128k context]

# DeepSeek Latest Models:
# - deepseek-chat (2024/12/26) [Latest V3 model]
# - deepseek-reasoner (2025/01/20) [Latest R1 model]
# Max tokens: 8192, supports JSON mode, function calling, context caching

# Groq Latest Models:
# - llama-3.3-70b-versatile [Latest Llama 3.3 for general use]
# - llama-3.3-70b-specdec [Latest Llama 3.3 for specialized tasks]
# - llama-3.1-8b-instant [Smaller, faster Llama 3.1 model]
# - llama-3.2-90b-vision-preview [Latest vision-enabled Llama model]

# Hugging Face Latest Models:
# - meta-llama/Llama-3.1-70B-Instruct [Latest Llama 3.1]

# Optional Configuration
# OUTPUT_FILE=custom_output.jsonl 

# Ollama Configuration
# No API key needed - install locally from https://ollama.ai
# Ollama will be auto-detected if running locally

# Optional Settings
# Uncomment and modify if needed

# Default provider to use (openai, deepseek, groq, ollama)
#DEFAULT_PROVIDER=openai

# Default model for each provider
#OPENAI_DEFAULT_MODEL=gpt-4o-mini
#DEEPSEEK_DEFAULT_MODEL=deepseek-chat
#GROQ_DEFAULT_MODEL=llama-3.3-70b-versatile

# Rate limiting settings (requests per minute)
#OPENAI_RATE_LIMIT=3500
#DEEPSEEK_RATE_LIMIT=100
#GROQ_RATE_LIMIT=1000

# Batch processing settings
#MAX_BATCH_SIZE=20
#MIN_BATCH_SIZE=5
#DEFAULT_BATCH_SIZE=10 